# LangGraph LongMemEval Benchmarker Configuration

# Dataset configuration
longmemeval_repo_path = "/Users/deesam/workspace/LongMemEval"
dataset_variant = "longmemeval_s.json"  # longmemeval_s.json, longmemeval_m.json, longmemeval_oracle.json

# MCP configuration  
mcp_url = "http://localhost:11546/mcp"
mcp_transport = "streamable_http"

# Model configuration
memory_agent_model = "gpt-4o-mini-2024-07-18"
qa_agent_model = "gpt-4o-2024-08-06"
evaluation_model = "anthropic.claude-3-haiku-20240307-v1:0"

# AWS Bedrock configuration (optional)
aws_region = "us-east-1"
# aws_access_key_id = "your_access_key"
# aws_secret_access_key = "your_secret_key"

# Benchmark configuration
vault_name = "longmemeval-benchmark"
output_dir = "./results"
max_concurrent_conversations = 1
enable_debug_logging = false